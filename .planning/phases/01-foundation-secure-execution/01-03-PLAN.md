---
phase: 01-foundation-secure-execution
plan: 03
type: execute
wave: 1
depends_on: []
gap_closure: true
files_modified:
  - src/agent/tools/sandbox.py
  - src/agent/prompts.py
autonomous: true
requirements: [SEC-01, ENG-01]
---

# Plan 01-03: Fix HITL Architecture — Decouple Code Generation from OI Execution

## Goal

The Pydantic AI agent must return a `CodeExecutionRequest` (with reasoning + code blocks) to `main.py` BEFORE any code runs. Currently `run_system_task` calls `interpreter.chat()` which runs Open Interpreter's own HITL loop internally — the approval happens inside OI, never surfacing to Pydantic AI's layer.

**Gap:** "Agent shows reasoning message before proposed code block" — UAT test 5 failed because `CodeExecutionRequest` is never returned; `main.py`'s reasoning display is never reached.

## Root Cause

`interpreter.chat(task)` in `SandboxedTool.run_system_task()` executes OI's full pipeline (LLM → prompt user → execute). By the time it returns, code has already run. `run_system_task` returns a plain string, so `main.py` hits the `else` branch and skips reasoning/code display.

## Fix

Replace `interpreter.chat()` with a direct Gemini API call to generate code blocks. Store them as `pending_blocks`. Return `CodeExecutionRequest` to Pydantic AI. Keep `confirm_execution` using `interpreter.computer.run()` for Docker-backed execution.

## Tasks

### Task 1: Rewrite `run_system_task` to use direct LLM for code generation

**File:** `src/agent/tools/sandbox.py`

Replace `interpreter.chat(task)` with a direct call to `google-genai` SDK to ask the LLM: "Generate shell or python code to: {task}. Return ONLY the code, no explanation."

Parse the response to extract code blocks (detect language from fences). Store as `pending_blocks`. Return `CodeExecutionRequest(reasoning=..., blocks=[...])`.

Set `interpreter.auto_run = True` so OI's own HITL doesn't interfere when `confirm_execution` calls `interpreter.computer.run()`.

Use the `GEMINI_API_KEY` env var and model `gemini-3-flash-preview`.

### Task 2: Update system prompt to instruct agent to always call `run_system_task` for system tasks

**File:** `src/agent/prompts.py`

Ensure the prompt clearly tells the agent:
- Always use `run_system_task` for any OS/shell/file task
- The tool returns a `CodeExecutionRequest` — the agent must return that object directly as its result (not summarize it)
- The agent should NOT attempt to execute code itself

### Task 3: Verify end-to-end flow

Run `python3 src/main.py` (with venv activated), ask "list files in current directory".

Verify:
1. Agent calls `run_system_task`
2. main.py prints `Agent Logic:` reasoning section
3. main.py prints `--- Proposed Code ---` section
4. main.py prompts `Approve execution? (y/n):`
5. After `y`, execution happens in Docker sandbox

## Success Criteria

- `main.py` displays `Agent Logic:` text before showing proposed code
- `main.py` displays `--- Proposed Code ---` with code block
- `main.py` prompts for approval before any execution
- After approval, code runs in Docker container
