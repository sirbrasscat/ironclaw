---
phase: 05-local-model-support-via-ollama
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/agent/provider.py
  - requirements.txt
autonomous: true
requirements:
  - ENG-05
must_haves:
  truths:
    - "PROVIDER=ollama reads OLLAMA_MODEL, OLLAMA_AGENT_MODEL, OLLAMA_CODEGEN_MODEL, OLLAMA_BASE_URL from env"
    - "Provider module can ping Ollama health endpoint and report reachability"
    - "Provider module can list locally pulled models and check if a given model tag is available"
    - "Provider config object exposes resolved agent_model and codegen_model names"
  artifacts:
    - path: "src/agent/provider.py"
      provides: "ProviderConfig dataclass, get_provider_config(), check_ollama_health()"
      exports: ["ProviderConfig", "get_provider_config", "check_ollama_health", "OllamaUnavailableError"]
    - path: "requirements.txt"
      provides: "ollama package dependency"
      contains: "ollama"
  key_links:
    - from: "src/agent/provider.py"
      to: "Ollama /api/tags endpoint"
      via: "httpx.AsyncClient GET in check_ollama_health()"
      pattern: "httpx\\.AsyncClient"
---

<objective>
Create a provider configuration module that centralises all Ollama/cloud provider env-var reading, health checking, and model resolution. This is the shared contract that both the Pydantic AI agent (core.py) and the code-generation step (sandbox.py) will depend on in Wave 2.

Purpose: Prevent duplicated env-var logic scattered across core.py and sandbox.py. Single place to change provider defaults.
Output: src/agent/provider.py with ProviderConfig dataclass, requirements.txt updated with ollama library.
</objective>

<execution_context>
@/home/brassy/.claude/get-shit-done/workflows/execute-plan.md
@/home/brassy/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/05-local-model-support-via-ollama/05-CONTEXT.md
</context>

<interfaces>
<!-- Contracts this plan creates. Wave 2 plans depend on these. -->

src/agent/provider.py must export:

```python
import os
from dataclasses import dataclass, field
from typing import Optional

VALID_PROVIDERS = {"ollama", "gemini", "anthropic", "openai"}

@dataclass
class ProviderConfig:
    provider: str                  # "ollama" | "gemini" | "anthropic" | "openai"
    # Ollama specifics (None when provider != "ollama")
    ollama_base_url: str           # default: "http://localhost:11434"
    ollama_agent_model: str        # resolved: OLLAMA_AGENT_MODEL or OLLAMA_MODEL
    ollama_codegen_model: str      # resolved: OLLAMA_CODEGEN_MODEL or OLLAMA_MODEL
    # Cloud specifics (None when provider == "ollama")
    gemini_api_key: Optional[str]
    anthropic_api_key: Optional[str]
    openai_api_key: Optional[str]

def get_provider_config() -> ProviderConfig:
    """Read env vars and resolve provider config. Raises ValueError for unknown PROVIDER."""
    ...

class OllamaUnavailableError(Exception):
    """Raised when Ollama health check fails or model not pulled."""
    pass

async def check_ollama_health(config: ProviderConfig) -> tuple[bool, list[str]]:
    """
    Ping Ollama /api/tags endpoint.
    Returns (reachable: bool, pulled_models: list[str]).
    Uses httpx async (not requests) for compatibility with async callers.
    """
    ...

def get_missing_models(config: ProviderConfig, pulled_models: list[str]) -> list[str]:
    """Return list of model names in config that are NOT in pulled_models."""
    ...
```
</interfaces>

<tasks>

<task type="auto">
  <name>Task 1: Add ollama to requirements.txt</name>
  <files>requirements.txt</files>
  <action>
    Add two lines to requirements.txt:
    - `ollama` — the official Ollama Python library (used for listing models; ollama.list())
    - `httpx` — async HTTP client used in check_ollama_health() to ping OLLAMA_BASE_URL/api/tags

    The current requirements.txt has: pydantic-ai, open-interpreter, docker, sqlalchemy, aiosqlite, pydantic-settings, pytest, chainlit, python-dotenv.

    Append `ollama` and `httpx` as new lines. Do not remove or reorder existing entries.
  </action>
  <verify>
    File contains lines with `ollama` and `httpx`.
    grep -E "^ollama$|^httpx$" requirements.txt
  </verify>
  <done>requirements.txt has `ollama` and `httpx` entries so pip install -r requirements.txt will install them.</done>
</task>

<task type="auto">
  <name>Task 2: Create src/agent/provider.py — provider config and Ollama health check</name>
  <files>src/agent/provider.py</files>
  <action>
    Create src/agent/provider.py with the following implementation:

    **ProviderConfig dataclass** — holds resolved provider settings:
    - `provider: str` — one of "ollama", "gemini", "anthropic", "openai"
    - `ollama_base_url: str` — defaults to "http://localhost:11434"
    - `ollama_agent_model: str` — resolved from OLLAMA_AGENT_MODEL or OLLAMA_MODEL (fallback "llama3.2")
    - `ollama_codegen_model: str` — resolved from OLLAMA_CODEGEN_MODEL or OLLAMA_MODEL (fallback "llama3.2")
    - `gemini_api_key: Optional[str]`
    - `anthropic_api_key: Optional[str]`
    - `openai_api_key: Optional[str]`

    **get_provider_config() -> ProviderConfig** — reads env vars:
    - PROVIDER env var (case-insensitive) selects the provider
    - If PROVIDER not set: auto-detect using existing fallback chain — GEMINI_API_KEY -> ANTHROPIC_API_KEY -> OPENAI_API_KEY -> default "gemini"
    - If PROVIDER set to unknown value: raise ValueError with message listing valid values
    - Resolve ollama_agent_model: OLLAMA_AGENT_MODEL if set, else OLLAMA_MODEL if set, else "llama3.2"
    - Resolve ollama_codegen_model: OLLAMA_CODEGEN_MODEL if set, else OLLAMA_MODEL if set, else "llama3.2"
    - OLLAMA_BASE_URL defaults to "http://localhost:11434"

    **OllamaUnavailableError(Exception)** — sentinel exception for startup checks.

    **async check_ollama_health(config: ProviderConfig) -> tuple[bool, list[str]]**:
    - Use httpx.AsyncClient to GET {config.ollama_base_url}/api/tags with a 5-second timeout
    - On success (HTTP 200): parse JSON, extract model names from response["models"][*]["name"], return (True, names_list)
    - On connection error or timeout: return (False, [])
    - Do NOT raise — callers decide what to do with the result

    **get_missing_models(config: ProviderConfig, pulled_models: list[str]) -> list[str]**:
    - Collect {config.ollama_agent_model, config.ollama_codegen_model} (deduplicated)
    - Return the subset not present in pulled_models
    - Match by exact string equality (Ollama tags are exact: "llama3.2", "mistral:7b")

    **provider_banner(config: ProviderConfig) -> str** — returns a human-readable startup line:
    - Ollama: "[*] Provider: Ollama — agent: {ollama_agent_model}, codegen: {ollama_codegen_model} ({ollama_base_url})"
    - Gemini: "[*] Provider: Gemini (cloud)"
    - Anthropic: "[*] Provider: Anthropic (cloud)"
    - OpenAI: "[*] Provider: OpenAI (cloud)"

    Use standard library only plus httpx (no other new imports). The file must be importable without side effects (no code running at module level beyond class/function definitions).
  </action>
  <verify>
    python3 -c "
    import sys; sys.path.insert(0, '/home/brassy/github/ironclaw')
    from src.agent.provider import ProviderConfig, get_provider_config, check_ollama_health, get_missing_models, provider_banner, OllamaUnavailableError
    import os
    os.environ['PROVIDER'] = 'gemini'
    os.environ['GEMINI_API_KEY'] = 'test-key'
    cfg = get_provider_config()
    assert cfg.provider == 'gemini'
    assert cfg.gemini_api_key == 'test-key'
    os.environ['PROVIDER'] = 'ollama'
    os.environ['OLLAMA_MODEL'] = 'mistral:7b'
    cfg2 = get_provider_config()
    assert cfg2.provider == 'ollama'
    assert cfg2.ollama_agent_model == 'mistral:7b'
    assert cfg2.ollama_codegen_model == 'mistral:7b'
    assert get_missing_models(cfg2, ['llama3.2']) == ['mistral:7b']
    print('provider.py: all assertions passed')
    "
  </verify>
  <done>
    src/agent/provider.py exists and is importable. ProviderConfig, get_provider_config(), check_ollama_health(), get_missing_models(), provider_banner(), OllamaUnavailableError all exported. PROVIDER=ollama env var correctly routes; OLLAMA_MODEL fallback resolves both agent and codegen models; missing model detection works.
  </done>
</task>

</tasks>

<verification>
1. `requirements.txt` contains `ollama` and `httpx` entries.
2. `src/agent/provider.py` is importable with no side effects.
3. `get_provider_config()` resolves PROVIDER=ollama with model fallback chain correctly.
4. `check_ollama_health()` is an async function returning (bool, list[str]).
5. `provider_banner()` returns a non-empty string for all four provider values.
</verification>

<success_criteria>
- src/agent/provider.py created and importable
- ProviderConfig resolves OLLAMA_AGENT_MODEL/OLLAMA_CODEGEN_MODEL/OLLAMA_MODEL fallback correctly
- check_ollama_health() pings /api/tags and parses model list
- get_missing_models() correctly identifies models not yet pulled
- requirements.txt updated with ollama and httpx
</success_criteria>

<output>
After completion, create `.planning/phases/05-local-model-support-via-ollama/05-01-SUMMARY.md`
</output>
