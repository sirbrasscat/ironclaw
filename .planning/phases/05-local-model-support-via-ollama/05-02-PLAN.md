---
phase: 05-local-model-support-via-ollama
plan: 02
type: execute
wave: 2
depends_on:
  - 05-01
files_modified:
  - src/agent/core.py
  - src/agent/tools/sandbox.py
autonomous: true
requirements:
  - ENG-05
must_haves:
  truths:
    - "When PROVIDER=ollama, Pydantic AI agent uses OllamaModel not Google/Anthropic/OpenAI model"
    - "When PROVIDER=ollama, run_system_task calls Ollama for code generation instead of google-genai"
    - "HITL contract (CodeExecutionRequest) is preserved identically for Ollama provider"
    - "Ollama code generation streams tokens through the on_output callback progressively"
    - "Cloud provider path is unchanged when PROVIDER is not ollama"
    - "Mid-session Ollama connection failure surfaces a clear error and does not silently retry"
  artifacts:
    - path: "src/agent/core.py"
      provides: "Provider-aware ironclaw_agent construction"
      contains: "from src.agent.provider import get_provider_config"
    - path: "src/agent/tools/sandbox.py"
      provides: "Provider-aware run_system_task (Ollama streaming or Gemini)"
      contains: "from src.agent.provider import get_provider_config"
  key_links:
    - from: "src/agent/core.py"
      to: "src/agent/provider.py"
      via: "get_provider_config() at module load"
      pattern: "get_provider_config"
    - from: "src/agent/tools/sandbox.py"
      to: "src/agent/provider.py"
      via: "get_provider_config() in run_system_task"
      pattern: "get_provider_config"
    - from: "src/agent/tools/sandbox.py"
      to: "ollama python library"
      via: "ollama.generate(stream=True) for streaming code generation"
      pattern: "ollama\\.generate|import ollama"
---

<objective>
Wire the provider.py config into the two LLM call sites: core.py (Pydantic AI agent model selection) and sandbox.py (run_system_task code generation). When PROVIDER=ollama, all LLM traffic routes through Ollama with streaming enabled. Cloud paths remain untouched.

Purpose: Makes the agent fully functional with Ollama as a drop-in LLM backend.
Output: Modified core.py using provider-aware model selection; modified sandbox.py using streaming Ollama for code generation when configured; mid-session error handling for connection drops.
</objective>

<execution_context>
@/home/brassy/.claude/get-shit-done/workflows/execute-plan.md
@/home/brassy/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/05-local-model-support-via-ollama/05-CONTEXT.md
@.planning/phases/05-local-model-support-via-ollama/05-01-SUMMARY.md
</context>

<interfaces>
<!-- Current code this plan modifies. Read before implementing. -->

From src/agent/core.py (current model selection — to be replaced):
```python
# Current hardcoded selection (replace with provider-aware logic)
if os.environ.get("GEMINI_API_KEY"):
    default_model = 'google-gla:gemini-2.5-flash'
elif os.environ.get("ANTHROPIC_API_KEY"):
    default_model = 'anthropic:claude-3-5-sonnet-latest'
else:
    default_model = 'openai:gpt-4o'

ironclaw_agent = Agent(
    default_model,
    system_prompt=IRONCLAW_SYSTEM_PROMPT,
    deps_type=AgentDeps,
    output_type=Union[CodeExecutionRequest, str]
)
```

From src/agent/tools/sandbox.py (current code-gen call — Gemini-only, to be provider-aware):
```python
def run_system_task(self, task: str) -> Union[CodeExecutionRequest, str]:
    self.pending_blocks = []
    api_key = os.environ.get("GEMINI_API_KEY")
    client = genai.Client(api_key=api_key)
    response = client.models.generate_content(
        model="gemini-2.5-flash",
        contents=prompt,
    )
    text = response.text or ""
    # ... parse fences, return CodeExecutionRequest
```

From src/agent/provider.py (created in Plan 01):
```python
@dataclass
class ProviderConfig:
    provider: str
    ollama_base_url: str
    ollama_agent_model: str
    ollama_codegen_model: str
    gemini_api_key: Optional[str]
    anthropic_api_key: Optional[str]
    openai_api_key: Optional[str]

def get_provider_config() -> ProviderConfig: ...
```

Pydantic AI Ollama model string format (per pydantic-ai docs):
- Use model string: `"ollama:llama3.2"` (pydantic-ai's built-in Ollama support)
- Or: `from pydantic_ai.models.ollama import OllamaModel` if string form is insufficient
- The base URL is set via OLLAMA_HOST env var OR by constructing OllamaModel with base_url param
</interfaces>

<tasks>

<task type="auto">
  <name>Task 1: Update src/agent/core.py for provider-aware model selection</name>
  <files>src/agent/core.py</files>
  <action>
    Replace the hardcoded model selection block in core.py with a provider-aware lookup using get_provider_config().

    Implementation steps:
    1. Add import: `from src.agent.provider import get_provider_config`
    2. Remove the existing `if os.environ.get("GEMINI_API_KEY"):` / elif block.
    3. Call `_provider_config = get_provider_config()` at module level (after load_dotenv has been called by the entry point — this is fine; core.py is imported after dotenv loads in main.py and web_ui.py).
    4. Build `default_model` string based on `_provider_config.provider`:
       - "ollama": Use pydantic-ai's Ollama model. Pydantic-ai supports Ollama via the `ollama:` prefix string format. Set `OLLAMA_HOST` env var to `_provider_config.ollama_base_url` before building the model string (pydantic-ai reads it), then use `default_model = f"ollama:{_provider_config.ollama_agent_model}"`.
       - "gemini": `default_model = 'google-gla:gemini-2.5-flash'` (unchanged)
       - "anthropic": `default_model = 'anthropic:claude-3-5-sonnet-latest'` (unchanged)
       - "openai": `default_model = 'openai:gpt-4o'` (unchanged)
    5. The `ironclaw_agent = Agent(default_model, ...)` line stays exactly as-is.

    IMPORTANT: Do NOT remove the `os` import. Do NOT change AgentDeps, tool registrations, or anything else.

    Note: If pydantic-ai's string-prefix Ollama support requires a different import or extra constructor kwargs, check what `pydantic_ai.models` exposes. The safest fallback is: `from pydantic_ai.models.ollama import OllamaModel` and pass `OllamaModel(model_name=..., base_url=...)` directly to Agent(). Try the string prefix first; use OllamaModel constructor if the string form raises ImportError or AttributeError at import time.
  </action>
  <verify>
    python3 -c "
    import sys, os
    sys.path.insert(0, '/home/brassy/github/ironclaw')
    os.environ['PROVIDER'] = 'gemini'
    os.environ['GEMINI_API_KEY'] = 'fake-key'
    from src.agent.core import ironclaw_agent, AgentDeps
    print('core.py import OK, model:', ironclaw_agent.model)
    "
  </verify>
  <done>
    core.py imports cleanly. ironclaw_agent.model reflects the provider selected by PROVIDER env var. Cloud provider paths produce the same model strings as before when PROVIDER is not "ollama".
  </done>
</task>

<task type="auto">
  <name>Task 2: Update src/agent/tools/sandbox.py for provider-aware streaming code generation</name>
  <files>src/agent/tools/sandbox.py</files>
  <action>
    Make run_system_task() route to Ollama (streaming) or Gemini based on provider config. The execution path (confirm_execution) is unchanged — it always uses interpreter.computer.run() with the Docker languages regardless of provider.

    Implementation in SandboxedTool.run_system_task():
    1. Add imports at top of file: `from src.agent.provider import get_provider_config` and `import ollama as ollama_lib`
    2. At the start of run_system_task(), call `config = get_provider_config()`
    3. Branch on config.provider:

    **Ollama branch** (`config.provider == "ollama"`):
    - Set `os.environ["OLLAMA_HOST"] = config.ollama_base_url` before the call (ollama library reads this)
    - Call with streaming enabled: `stream = ollama_lib.generate(model=config.ollama_codegen_model, prompt=prompt, stream=True)`
    - Iterate over the stream, accumulating text and forwarding tokens to on_output if provided:
      ```python
      text = ""
      for chunk in stream:
          token = chunk.get("response", "")
          text += token
          if on_output:
              on_output(token)
      ```
    - After the loop, proceed to fence parsing with the accumulated `text`
    - `run_system_task` signature already accepts `on_output` callback — pass it through from the caller

    **Gemini branch** (all other providers, existing code):
    - Keep the existing `api_key = os.environ.get("GEMINI_API_KEY")` / `genai.Client` / `generate_content()` block as-is
    - This handles "gemini", "anthropic", "openai" — existing Gemini codegen fallback behavior unchanged

    **Shared code**: The fence-parsing regex block and the CodeExecutionRequest construction are identical for both branches — factor them into a helper method `_parse_code_blocks(self, text: str) -> List[CodeBlock]` to avoid duplication.

    Also update SandboxedTool.__init__(): Keep existing `interpreter.llm.model` and `interpreter.llm.api_key` lines as-is — they only affect OI's own chat loop which we've disabled. They will not run during Ollama usage.

    Do NOT change confirm_execution() — it uses interpreter.computer.run() which is Docker-backed and provider-agnostic.
  </action>
  <verify>
    python3 -c "
    import sys, os
    sys.path.insert(0, '/home/brassy/github/ironclaw')
    os.environ['PROVIDER'] = 'gemini'
    os.environ['GEMINI_API_KEY'] = 'fake-key'
    from src.agent.tools.sandbox import run_system_task, confirm_execution, CodeExecutionRequest
    print('sandbox.py import OK')
    print('CodeExecutionRequest fields:', list(CodeExecutionRequest.model_fields.keys()))
    "
  </verify>
  <done>
    sandbox.py imports cleanly. run_system_task routes to ollama_lib.generate(stream=True) when PROVIDER=ollama, accumulating tokens and forwarding each to on_output callback. Gemini path unchanged. _parse_code_blocks helper extracted. confirm_execution unchanged. CodeExecutionRequest contract preserved.
  </done>
</task>

<task type="auto">
  <name>Task 3: Add mid-session Ollama connection error handling in sandbox.py</name>
  <files>src/agent/tools/sandbox.py</files>
  <action>
    Wrap the Ollama streaming call in run_system_task() with error handling so mid-session connection drops surface a clear message and do not silently retry or fall back to another provider (locked decision).

    In the Ollama branch of run_system_task(), wrap both the ollama_lib.generate() call AND the streaming iteration in a try/except block:

    ```python
    try:
        stream = ollama_lib.generate(
            model=config.ollama_codegen_model,
            prompt=prompt,
            stream=True,
        )
        text = ""
        for chunk in stream:
            token = chunk.get("response", "")
            text += token
            if on_output:
                on_output(token)
    except Exception as e:
        raise OllamaUnavailableError(
            f"Ollama connection failed during code generation: {e}\n"
            f"Check that Ollama is running at {config.ollama_base_url}"
        ) from e
    ```

    Import OllamaUnavailableError at the top of sandbox.py:
    `from src.agent.provider import get_provider_config, OllamaUnavailableError`

    The OllamaUnavailableError will propagate up through run_system_task -> the Pydantic AI tool -> ironclaw_agent.run_stream(), where the caller (main.py or web_ui.py) will catch it and display the error to the user. Do NOT swallow the exception or retry.

    No changes to confirm_execution() — Docker execution errors are handled separately.
  </action>
  <verify>
    python3 -c "
    import sys, os
    sys.path.insert(0, '/home/brassy/github/ironclaw')
    os.environ['PROVIDER'] = 'ollama'
    os.environ['OLLAMA_MODEL'] = 'llama3.2'
    from src.agent.tools.sandbox import SandboxedTool
    from src.agent.provider import OllamaUnavailableError
    import inspect
    src = inspect.getsource(SandboxedTool.run_system_task)
    assert 'OllamaUnavailableError' in src, 'OllamaUnavailableError not found in run_system_task'
    assert 'stream=True' in src, 'stream=True not found'
    print('Error handling and streaming verified in sandbox.py')
    "
  </verify>
  <done>
    run_system_task Ollama branch uses stream=True and raises OllamaUnavailableError (not a generic Exception) on any connection failure. No silent retry or fallback. The error message includes the base URL for easy diagnosis.
  </done>
</task>

</tasks>

<verification>
1. core.py imports with PROVIDER=gemini without error.
2. core.py imports with PROVIDER=ollama without error (pydantic-ai Ollama model string accepted).
3. sandbox.py imports cleanly regardless of PROVIDER.
4. sandbox.py Ollama branch uses stream=True and forwards tokens to on_output callback.
5. sandbox.py raises OllamaUnavailableError (not silent) on connection failure in Ollama branch.
6. The fence-parsing logic is shared (_parse_code_blocks or equivalent), not duplicated.
7. confirm_execution() is functionally unchanged (no Docker/interpreter changes).
</verification>

<success_criteria>
- PROVIDER=ollama: ironclaw_agent uses "ollama:{OLLAMA_AGENT_MODEL}" model
- PROVIDER=ollama: run_system_task uses ollama_lib.generate(stream=True) with token forwarding
- PROVIDER=gemini (default): all behavior identical to pre-phase code
- CodeExecutionRequest returned identically by both branches
- Mid-session Ollama failure raises OllamaUnavailableError with clear message
- No changes to confirm_execution / Docker execution path
</success_criteria>

<output>
After completion, create `.planning/phases/05-local-model-support-via-ollama/05-02-SUMMARY.md`
</output>
