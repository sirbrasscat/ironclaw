---
phase: 05-local-model-support-via-ollama
plan: 03
type: execute
wave: 2
depends_on:
  - 05-01
  - 05-02
files_modified:
  - src/main.py
  - src/web_ui.py
autonomous: false
requirements:
  - ENG-05
must_haves:
  truths:
    - "CLI prints provider banner on startup showing active provider and model names"
    - "CLI with PROVIDER=ollama pings Ollama health; if unreachable prompts user to fall back to cloud or abort"
    - "CLI exits with pull command if required models are not pulled"
    - "Web UI welcome message includes provider banner"
    - "Web UI with PROVIDER=ollama shows AskActionMessage fallback button if Ollama is unreachable"
    - "Web UI exits startup path with pull command message if required models are not pulled"
  artifacts:
    - path: "src/main.py"
      provides: "Startup Ollama health check, fallback prompt, provider banner print"
      contains: "check_ollama_health"
    - path: "src/web_ui.py"
      provides: "Startup Ollama health check, AskActionMessage fallback, provider banner in welcome"
      contains: "check_ollama_health"
  key_links:
    - from: "src/main.py"
      to: "src/agent/provider.py"
      via: "check_ollama_health(), provider_banner(), get_missing_models()"
      pattern: "check_ollama_health|provider_banner"
    - from: "src/web_ui.py"
      to: "src/agent/provider.py"
      via: "check_ollama_health(), provider_banner(), get_missing_models()"
      pattern: "check_ollama_health|provider_banner"
---

<objective>
Wire the provider health check and banner into both entry points. When PROVIDER=ollama, startup verifies Ollama is reachable and models are pulled before proceeding. Both CLI and Web UI display the active provider on startup and offer an interactive fallback if Ollama is unavailable.

Purpose: Fulfill the locked startup behaviour decisions — check_ollama_health() is called but currently never invoked by either entry point.
Output: Modified src/main.py and src/web_ui.py with startup health checks, provider banners, and interactive fallback flows.
</objective>

<execution_context>
@/home/brassy/.claude/get-shit-done/workflows/execute-plan.md
@/home/brassy/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/phases/05-local-model-support-via-ollama/05-CONTEXT.md
@.planning/phases/05-local-model-support-via-ollama/05-01-SUMMARY.md
@.planning/phases/05-local-model-support-via-ollama/05-02-SUMMARY.md
</context>

<interfaces>
<!-- Current entry point structure and provider.py contracts the executor needs. -->

From src/main.py (current startup — to be extended):
```python
async def main(session_id: str = "default"):
    print("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
    print(" IronClaw Agent System - Phase 2 Persistence")
    print("━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━")
    print(f"Session: {session_id}")
    print("Type 'exit' or 'quit' to stop.")
    # ... db init, history load, while loop

if __name__ == "__main__":
    if not any(os.environ.get(k) for k in ["GEMINI_API_KEY", "OPENAI_API_KEY", "ANTHROPIC_API_KEY"]):
        print("Error: No API key found. ...")
    else:
        # ... argparse, asyncio.run(main(...))
```

From src/web_ui.py (current on_chat_start — to be extended):
```python
@cl.on_chat_start
async def on_chat_start():
    # sets up session, loads history, sends welcome message
```

From src/agent/provider.py (Plan 01 contracts):
```python
def get_provider_config() -> ProviderConfig: ...
async def check_ollama_health(config: ProviderConfig) -> tuple[bool, list[str]]: ...
def get_missing_models(config: ProviderConfig, pulled_models: list[str]) -> list[str]: ...
def provider_banner(config: ProviderConfig) -> str: ...
class OllamaUnavailableError(Exception): ...
```
</interfaces>

<tasks>

<task type="auto">
  <name>Task 1: Add Ollama startup check and provider banner to src/main.py</name>
  <files>src/main.py</files>
  <action>
    Extend main.py to print the provider banner and perform an Ollama health check on startup when PROVIDER=ollama.

    **Imports to add** (after the existing imports, before the async def main):
    ```python
    from src.agent.provider import get_provider_config, check_ollama_health, get_missing_models, provider_banner
    ```

    **In the `main()` async function**, directly after the banner print block (the three `print()` lines with the separator), add:

    ```python
    # Provider banner
    _cfg = get_provider_config()
    print(provider_banner(_cfg))

    # Ollama startup health check
    if _cfg.provider == "ollama":
        reachable, pulled_models = await check_ollama_health(_cfg)
        if not reachable:
            answer = input(
                f"Ollama unavailable at {_cfg.ollama_base_url}. Fall back to cloud? [y/N] "
            ).strip().lower()
            if answer not in ("y", "yes"):
                print("Aborting. Start Ollama and retry, or set PROVIDER to a cloud provider.")
                return
            else:
                # Re-resolve config without Ollama (unset PROVIDER, let fallback chain run)
                import os as _os
                _os.environ.pop("PROVIDER", None)
                _cfg = get_provider_config()
                print(provider_banner(_cfg))
        else:
            missing = get_missing_models(_cfg, pulled_models)
            if missing:
                for m in missing:
                    print(f"[!] Model not pulled: {m}. Run: ollama pull {m}")
                print("Aborting. Pull the required models and retry.")
                return
    ```

    **Update the `__main__` guard**: Remove the existing cloud-key-only guard:
    ```python
    if not any(os.environ.get(k) for k in ["GEMINI_API_KEY", "OPENAI_API_KEY", "ANTHROPIC_API_KEY"]):
        print("Error: No API key found. ...")
    ```
    Replace it with a simpler check that also allows PROVIDER=ollama (which needs no API key):
    ```python
    _startup_cfg = get_provider_config()
    if _startup_cfg.provider != "ollama" and not any(
        os.environ.get(k) for k in ["GEMINI_API_KEY", "OPENAI_API_KEY", "ANTHROPIC_API_KEY"]
    ):
        print("Error: No API key found. Set GEMINI_API_KEY, OPENAI_API_KEY, ANTHROPIC_API_KEY, or PROVIDER=ollama in .env")
    else:
        parser = argparse.ArgumentParser(description="IronClaw Agent CLI")
        parser.add_argument("--session", type=str, default="default", help="Session ID to load/create")
        args = parser.parse_args()
        asyncio.run(main(args.session))
    ```

    Do NOT change the while loop, history loading, or agent call logic.
  </action>
  <verify>
    python3 -c "
    import sys, os
    sys.path.insert(0, '/home/brassy/github/ironclaw')
    os.environ['PROVIDER'] = 'gemini'
    os.environ['GEMINI_API_KEY'] = 'fake-key'
    import ast
    src = open('/home/brassy/github/ironclaw/src/main.py').read()
    tree = ast.parse(src)
    assert 'check_ollama_health' in src, 'check_ollama_health missing'
    assert 'provider_banner' in src, 'provider_banner missing'
    assert 'Fall back to cloud' in src, 'fallback prompt missing'
    assert 'ollama pull' in src, 'pull command hint missing'
    print('main.py startup wiring verified')
    "
  </verify>
  <done>
    main.py prints provider_banner() on startup. When PROVIDER=ollama: pings health, prompts "Fall back to cloud? [y/N]" if unreachable, prints "ollama pull {model}" and aborts if models missing. Cloud entry guard updated to allow PROVIDER=ollama without API keys.
  </done>
</task>

<task type="auto">
  <name>Task 2: Add Ollama startup check and provider banner to src/web_ui.py</name>
  <files>src/web_ui.py</files>
  <action>
    Extend web_ui.py's on_chat_start handler to display a provider banner and run the Ollama health check. If Ollama is unreachable, show an AskActionMessage with fallback/abort options.

    **Imports to add** at the top of web_ui.py (after existing imports):
    ```python
    from src.agent.provider import get_provider_config, check_ollama_health, get_missing_models, provider_banner
    ```

    **In the `on_chat_start()` async function**, find where the welcome message is sent (a `cl.Message(content=...)` or similar). Add the provider banner to the welcome message content. For example, if the current welcome is:

    ```python
    await cl.Message(content="Welcome to IronClaw...").send()
    ```

    Change it to:

    ```python
    _cfg = get_provider_config()
    _banner = provider_banner(_cfg)
    await cl.Message(content=f"Welcome to IronClaw\n\n{_banner}").send()
    ```

    Then, still inside on_chat_start(), add the Ollama health check AFTER the welcome message:

    ```python
    if _cfg.provider == "ollama":
        reachable, pulled_models = await check_ollama_health(_cfg)
        if not reachable:
            action_res = await cl.AskActionMessage(
                content=(
                    f"Ollama is unreachable at {_cfg.ollama_base_url}. "
                    "Fall back to a cloud provider, or abort?"
                ),
                actions=[
                    cl.Action(name="fallback", label="Fall back to cloud", payload={"choice": "fallback"}),
                    cl.Action(name="abort", label="Abort session", payload={"choice": "abort"}),
                ],
            ).send()
            choice = (action_res.get("payload", {}) if action_res else {}).get("choice", "abort")
            if choice == "fallback":
                import os as _os
                _os.environ.pop("PROVIDER", None)
                _cfg = get_provider_config()
                await cl.Message(content=f"Switched provider. {provider_banner(_cfg)}").send()
            else:
                await cl.Message(content="Session aborted. Start Ollama and refresh.").send()
                return
        else:
            missing = get_missing_models(_cfg, pulled_models)
            if missing:
                pull_cmds = "\n".join(f"  ollama pull {m}" for m in missing)
                await cl.Message(
                    content=f"Required models not pulled. Run:\n```\n{pull_cmds}\n```\nThen refresh."
                ).send()
                return
    ```

    Note on AskActionMessage API: In Chainlit 2.x the payload field on the returned action is in `action_res["payload"]` when action_res is not None. Use `.get()` defensively in case the user closes without choosing.

    Do NOT change the session setup, history loading, or message handler logic outside on_chat_start.
  </action>
  <verify>
    python3 -c "
    import ast
    src = open('/home/brassy/github/ironclaw/src/web_ui.py').read()
    assert 'check_ollama_health' in src, 'check_ollama_health missing from web_ui.py'
    assert 'provider_banner' in src, 'provider_banner missing from web_ui.py'
    assert 'AskActionMessage' in src, 'AskActionMessage fallback missing from web_ui.py'
    assert 'Fall back' in src, 'fallback label missing'
    assert 'ollama pull' in src, 'pull command hint missing'
    ast.parse(src)
    print('web_ui.py startup wiring verified and parses OK')
    "
  </verify>
  <done>
    web_ui.py on_chat_start shows provider_banner() in the welcome message. When PROVIDER=ollama: pings health, shows AskActionMessage with "Fall back to cloud" / "Abort session" buttons if unreachable, shows pull commands and returns early if models missing.
  </done>
</task>

</tasks>

<verification>
1. src/main.py contains check_ollama_health, provider_banner, fallback prompt, and ollama pull hint.
2. src/web_ui.py contains check_ollama_health, provider_banner in welcome, AskActionMessage fallback.
3. PROVIDER=gemini: banner prints on CLI startup; no Ollama ping.
4. PROVIDER=ollama with reachable Ollama and pulled models: banner prints, session proceeds normally.
5. Both files parse without syntax errors: python3 -c "import ast; ast.parse(open('src/main.py').read()); ast.parse(open('src/web_ui.py').read()); print('OK')"
</verification>

<success_criteria>
- CLI startup always prints provider_banner()
- CLI PROVIDER=ollama: health check -> fallback prompt if unreachable -> pull hint if missing
- Web UI on_chat_start always shows provider banner in welcome message
- Web UI PROVIDER=ollama: health check -> AskActionMessage if unreachable -> pull hint if missing
- Cloud provider startup path unchanged (no Ollama ping)
</success_criteria>

<output>
After completion, create `.planning/phases/05-local-model-support-via-ollama/05-03-SUMMARY.md`
</output>
