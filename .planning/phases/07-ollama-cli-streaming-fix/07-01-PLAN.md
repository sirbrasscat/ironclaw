---
phase: 07-ollama-cli-streaming-fix
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/main.py
  - src/agent/tools/sandbox.py
autonomous: true
requirements:
  - ENG-05

must_haves:
  truths:
    - Running `python3 src/main.py` with PROVIDER=ollama prints tokens progressively to the terminal during code generation (no waiting for the full response before output appears).
    - The approval prompt ("Approve execution? (y/n):") starts on its own line after streaming completes.
    - The confirmation call also passes `deps=AgentDeps(on_output=_callback)` — Docker execution output streams for Ollama the same way.
    - Non-Ollama providers (Gemini, Anthropic, OpenAI) are unaffected — their behavior is unchanged.
    - `SandboxedTool.__init__` contains no reference to `interpreter.llm.model` or `interpreter.llm.api_key`.
    - `interpreter.offline` is set to `True` in `SandboxedTool.__init__`.
  artifacts:
    - src/main.py (modified — AgentDeps import added, _callback + _deps defined before while loop, deps= passed to both ironclaw_agent.run() calls, trailing print() after planning call)
    - src/agent/tools/sandbox.py (modified — dead OI LLM config block removed, offline=True, one-liner comment added)
  key_links:
    - main.py _callback → AgentDeps(on_output=_callback) → ironclaw_agent.run(deps=_deps) → core.py ctx.deps.on_output → sandbox.py on_output(token)
---

<objective>
Wire `AgentDeps(on_output=callback)` into both `ironclaw_agent.run()` calls in `main.py` so Ollama tokens print progressively to the terminal during CLI code generation. Remove the dead Open Interpreter LLM configuration from `SandboxedTool.__init__` and set `interpreter.offline = True`.

Purpose: Closes GAP-01 (main.py missing deps=AgentDeps()) and GAP-02 (hardcoded interpreter.llm.model) from the v1.0 audit. The Ollama streaming infrastructure (token accumulation loop in sandbox.py, ctx.deps guard in core.py) was fully implemented in Phase 5 — this plan connects the missing wire at the call site.

Output: Two small surgical edits to existing files. No new files, no new dependencies.
</objective>

<execution_context>
@/home/brassy/.claude/get-shit-done/workflows/execute-plan.md
@/home/brassy/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/ROADMAP.md
@.planning/STATE.md

<interfaces>
<!-- Key types the executor needs — extracted from codebase. No exploration required. -->

From src/agent/core.py (line 37-39):
```python
@dataclass
class AgentDeps:
    on_output: Optional[Callable[[str], None]] = None
```

From src/agent/core.py (line 51-57) — already correct, NO changes needed in core.py:
```python
@ironclaw_agent.tool
def run_system_task(ctx: RunContext[AgentDeps], task: str) -> Union[CodeExecutionRequest, str]:
    on_output = ctx.deps.on_output if ctx.deps else None
    return _run_system_task(task, on_output=on_output)
```

From src/agent/core.py (line 59-65) — already correct, NO changes needed in core.py:
```python
@ironclaw_agent.tool
def confirm_execution(ctx: RunContext[AgentDeps]) -> str:
    on_output = ctx.deps.on_output if ctx.deps else None
    return _confirm_execution(on_output=on_output)
```

From src/agent/tools/sandbox.py (line 114-128) — already correct, NO changes needed in streaming loop:
```python
if config.provider == "ollama":
    os.environ["OLLAMA_HOST"] = config.ollama_base_url
    try:
        stream = ollama_lib.generate(
            model=config.ollama_codegen_model,
            prompt=prompt,
            stream=True,
        )
        text = ""
        for chunk in stream:
            token = chunk.get("response", "")
            text += token
            if on_output:
                on_output(token)   # ← already calls on_output per token
```

Current main.py import line (line 17) — needs AgentDeps added:
```python
from src.agent.core import ironclaw_agent, CodeExecutionRequest
```

Current main.py planning call (lines 79-83) — missing deps=:
```python
result = await ironclaw_agent.run(
    user_input,
    message_history=history,
    output_type=CodeExecutionRequest | str
)
```

Current main.py confirmation call (lines 103-107) — missing deps=:
```python
confirm_result = await ironclaw_agent.run(
    "Confirm the execution.",
    message_history=history,
    output_type=str
)
```

Current sandbox.py SandboxedTool.__init__ dead lines (lines 50-62):
```python
interpreter.computer.languages = [BoundDockerPython, BoundDockerShell]

# Configure Open Interpreter's LLM to use Gemini via LiteLLM
interpreter.llm.model = "gemini/gemini-2.5-flash"
interpreter.llm.api_key = os.environ.get("GEMINI_API_KEY")

# Configure interpreter behavior
# auto_run=True so OI's own HITL loop doesn't interfere when
# confirm_execution() calls interpreter.computer.run() directly.
# Our HITL is enforced at the Pydantic AI layer via CodeExecutionRequest.
interpreter.auto_run = True
interpreter.offline = False
interpreter.safe_mode = False
```
</interfaces>
</context>

<tasks>

<task type="auto">
  <name>Task 1: Wire AgentDeps streaming callback into main.py CLI loop</name>
  <files>src/main.py</files>
  <action>
Make three changes to src/main.py:

**1. Add AgentDeps to the import from src.agent.core (line 17):**

Change:
```python
from src.agent.core import ironclaw_agent, CodeExecutionRequest
```
To:
```python
from src.agent.core import ironclaw_agent, CodeExecutionRequest, AgentDeps
```

**2. Define the callback and deps object once, before the `while True:` loop.** Insert these two lines immediately before the `while True:` line (after the `if history:` block, around line 71):
```python
_callback = lambda token: print(token, end='', flush=True)
_deps = AgentDeps(on_output=_callback)
```
Do NOT define these inside the loop — they must be created once per session, not re-created on every iteration.

**3. Add `deps=_deps` to both ironclaw_agent.run() calls AND add a trailing `print()` after the planning call:**

Planning call — change:
```python
result = await ironclaw_agent.run(
    user_input,
    message_history=history,
    output_type=CodeExecutionRequest | str
)
```
To:
```python
result = await ironclaw_agent.run(
    user_input,
    message_history=history,
    output_type=CodeExecutionRequest | str,
    deps=_deps,
)
print()  # trailing newline after streaming so approval prompt starts on its own line
```

Confirmation call — change:
```python
confirm_result = await ironclaw_agent.run(
    "Confirm the execution.",
    message_history=history,
    output_type=str
)
```
To:
```python
confirm_result = await ironclaw_agent.run(
    "Confirm the execution.",
    message_history=history,
    output_type=str,
    deps=_deps,
)
```

Do NOT add a second trailing `print()` after the confirmation call — the confirmation result is already printed by `print(f"\nResult:\n{confirm_result.output}")`.

Do NOT add any provider-conditional logic. Cloud providers (Gemini, Anthropic, OpenAI) silently ignore `on_output` because their branch in `run_system_task` never calls it. The callback is harmless for all providers.
  </action>
  <verify>
    <automated>cd /home/brassy/github/ironclaw && python3 -c "
import ast, sys
with open('src/main.py') as f:
    src = f.read()
# Check AgentDeps imported
assert 'AgentDeps' in src, 'AgentDeps not imported'
# Check _callback defined
assert '_callback' in src, '_callback not defined'
# Check _deps defined
assert '_deps' in src, '_deps not defined'
# Check both run() calls have deps=_deps
assert src.count('deps=_deps') == 2, f'Expected 2 deps=_deps, found {src.count(\"deps=_deps\")}'
# Check trailing print() exists (bare print with no args after planning run)
assert 'print()' in src, 'trailing print() not found'
# Syntax check
ast.parse(src)
print('main.py: all checks passed')
"
    </automated>
  </verify>
  <done>main.py imports AgentDeps; _callback and _deps defined once before the while loop; both ironclaw_agent.run() calls include deps=_deps; a bare print() follows the planning run call; file is syntactically valid Python.</done>
</task>

<task type="auto">
  <name>Task 2: Remove dead OI LLM config from SandboxedTool.__init__ and set offline=True</name>
  <files>src/agent/tools/sandbox.py</files>
  <action>
Make the following surgical changes to `SandboxedTool.__init__` in src/agent/tools/sandbox.py.

**Remove these lines entirely** (lines 52-54 in the current file):
```python
        # Configure Open Interpreter's LLM to use Gemini via LiteLLM
        interpreter.llm.model = "gemini/gemini-2.5-flash"
        interpreter.llm.api_key = os.environ.get("GEMINI_API_KEY")
```

**After the language setup line** (`interpreter.computer.languages = [BoundDockerPython, BoundDockerShell]`), add the one-liner comment:
```python
        # OI used as execution engine only — LLM config not needed
```

**Change `interpreter.offline = False` to `interpreter.offline = True`.**

**Do NOT touch any other lines.** Specifically, leave these untouched:
- `interpreter.auto_run = True` — must remain True (disables OI's internal HITL loop)
- `interpreter.safe_mode = False` — must remain False
- The comment block above `interpreter.auto_run` (lines 57-59 in original) — leave it as-is

The target state of the `__init__` block after the language classes should look like:

```python
        interpreter.computer.languages = [BoundDockerPython, BoundDockerShell]
        # OI used as execution engine only — LLM config not needed

        # Configure interpreter behavior
        # auto_run=True so OI's own HITL loop doesn't interfere when
        # confirm_execution() calls interpreter.computer.run() directly.
        # Our HITL is enforced at the Pydantic AI layer via CodeExecutionRequest.
        interpreter.auto_run = True
        interpreter.offline = True
        interpreter.safe_mode = False
```
  </action>
  <verify>
    <automated>cd /home/brassy/github/ironclaw && python3 -c "
import ast
with open('src/agent/tools/sandbox.py') as f:
    src = f.read()
# Removed lines must not exist
assert 'interpreter.llm.model' not in src, 'interpreter.llm.model still present'
assert 'interpreter.llm.api_key' not in src, 'interpreter.llm.api_key still present'
assert 'gemini/gemini-2.5-flash' not in src, 'hardcoded model string still present'
# offline must be True
assert 'interpreter.offline = True' in src, 'interpreter.offline not set to True'
# auto_run and safe_mode must still be present
assert 'interpreter.auto_run = True' in src, 'interpreter.auto_run = True missing'
assert 'interpreter.safe_mode = False' in src, 'interpreter.safe_mode = False missing'
# One-liner comment must be present
assert 'OI used as execution engine only' in src, 'one-liner comment missing'
# Syntax check
ast.parse(src)
print('sandbox.py: all checks passed')
"
    </automated>
  </verify>
  <done>sandbox.py SandboxedTool.__init__ contains no interpreter.llm.model or interpreter.llm.api_key lines; interpreter.offline is True; one-liner comment present; auto_run=True and safe_mode=False untouched; file is syntactically valid Python.</done>
</task>

</tasks>

<verification>
Run both automated checks back-to-back to confirm both files are clean:

```bash
cd /home/brassy/github/ironclaw

# Task 1 check
python3 -c "
import ast
with open('src/main.py') as f:
    src = f.read()
assert 'AgentDeps' in src
assert '_callback' in src
assert '_deps' in src
assert src.count('deps=_deps') == 2
assert 'print()' in src
ast.parse(src)
print('main.py OK')
"

# Task 2 check
python3 -c "
import ast
with open('src/agent/tools/sandbox.py') as f:
    src = f.read()
assert 'interpreter.llm.model' not in src
assert 'interpreter.llm.api_key' not in src
assert 'interpreter.offline = True' in src
assert 'interpreter.auto_run = True' in src
assert 'interpreter.safe_mode = False' in src
assert 'OI used as execution engine only' in src
ast.parse(src)
print('sandbox.py OK')
"

# Import smoke test — confirms no import errors after edits
python3 -c "
import os, sys
sys.path.insert(0, '.')
os.environ.setdefault('GEMINI_API_KEY', 'fake')
from src.agent.core import ironclaw_agent, CodeExecutionRequest, AgentDeps
print('imports OK:', AgentDeps)
"
```

Manual smoke test (optional, requires PROVIDER=ollama and Ollama running):
```bash
PROVIDER=ollama python3 src/main.py
# Type a task — tokens should appear progressively, one per chunk, with no buffering
# Approval prompt must start on its own line after tokens finish
```
</verification>

<success_criteria>
1. `python3 src/main.py` with PROVIDER=ollama streams tokens progressively to the terminal — each token prints immediately as it arrives, not buffered until the full response is ready.
2. After streaming ends, the approval prompt `Approve execution? (y/n):` starts on its own line (trailing newline is present).
3. The confirmation `ironclaw_agent.run("Confirm the execution.")` call also receives `deps=_deps` — Docker execution output streams for the Ollama path.
4. Non-Ollama providers (Gemini, Anthropic, OpenAI) are unaffected — the callback is present but the Gemini branch in run_system_task never invokes it.
5. `SandboxedTool.__init__` contains no `interpreter.llm.model` or `interpreter.llm.api_key` lines. `interpreter.offline = True`.
6. Both src/main.py and src/agent/tools/sandbox.py parse cleanly as valid Python (`ast.parse()` passes).
</success_criteria>

<output>
After completion, create `.planning/phases/07-ollama-cli-streaming-fix/07-01-SUMMARY.md` with:
- What was changed in each file (brief)
- Verification results (pass/fail for each automated check)
- ENG-05 traceability note: Phase 7 closes GAP-01 and GAP-02; ENG-05 is now complete
- Any deviations from the plan (none expected)
</output>
